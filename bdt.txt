import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
Usage
Assuming environment variables are set as follows:

export JAVA_HOME=/usr/java/default
export PATH=${JAVA_HOME}/bin:${PATH}
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
Compile WordCount.java and create a jar:

$ bin/hadoop com.sun.tools.javac.Main WordCount.java
$ jar cf wc.jar WordCount*.class
Assuming that:

/user/joe/wordcount/input - input directory in HDFS
/user/joe/wordcount/output - output directory in HDFS
Sample text-files as input:

$ bin/hadoop fs -ls /user/joe/wordcount/input/
/user/joe/wordcount/input/file01
/user/joe/wordcount/input/file02

$ bin/hadoop fs -cat /user/joe/wordcount/input/file01
Hello World Bye World

$ bin/hadoop fs -cat /user/joe/wordcount/input/file02
Hello Hadoop Goodbye Hadoop
Run the application:

$ bin/hadoop jar wc.jar WordCount /user/joe/wordcount/input /user/joe/wordcount/output






import java.io.IOException;
//import java.util.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.IntWritable;
//import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
//import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
//import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.conf.Configuration;
//import MaximumTemp.MaxTemperatureMapper;
//import MaxTemp.MaxTemperatureReducer;

public class MaxTemp {
        // Mapper
        /*MaxTemperatureMapper class is static
        * and extends Mapper abstract class
        * having four Hadoop generics type
        * LongWritable, Text, Text, Text.
        */
        public static class MaxTemperatureMapper extends
        Mapper<LongWritable, Text, Text, IntWritable> {
        // the data in our data set with
        // this value is inconsistent data
        //public static final int MISSING = 9999;

        public void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {
        String line=value.toString();
        String year=line.substring(15,19);
        int airtemp;
        if(line.charAt(87)=='+')
        {
        airtemp=Integer.parseInt(line.substring(88,92));
        }
        else
        airtemp=Integer.parseInt(line.substring(87,92));
        String q=line.substring(92,93);
        if(airtemp!=9999 && q.matches("[01459]"))
        {

        context.write(new Text(year), new IntWritable(airtemp));
        }
        }
        }
        // Reducer
        /*MaxTemperatureReducer class is static
        and extends Reducer abstract class
        having four Hadoop generics type
        Text, Text, Text, Text.
        */
        public static class MaxTemperatureReducer extends
        Reducer<Text, IntWritable, Text, IntWritable> {
        /**
        * @method reduce
        * This method takes the input as key and
        * list of values pair from the mapper,
        * it does aggregation based on keys and
        * produces the final context.
        */
        public void reduce(Text key,Iterable<IntWritable> values, Context context)
        throws IOException, InterruptedException {
        int maxvalue= Integer.MIN_VALUE;
        for (IntWritable value : values) {
        maxvalue=Math.max(maxvalue, value.get());
        }
        context.write(key, new IntWritable(maxvalue));
        }
        }

        /**
        * @method main
        * This method is used for setting
        * all the configuration properties.
        * It acts as a driver for map-reduce
        * code.
        */
        public static void main(String[] args) throws Exception {
        // reads the default configuration of the
        // cluster from the configuration XML files
        Configuration conf = new Configuration();
        // Initializing the job with the
        // default configuration of the cluster
        // Job job = new Job(conf, "weather example");
        Job job = Job.getInstance(conf, "weather example");
        // Assigning the driver class name
        job.setJarByClass(MaxTemp.class);
        // Key type coming out of mapper
        // job.setMapOutputKeyClass(Text.class);
        // value type coming out of mapper
        // job.setMapOutputValueClass(Text.class);
        // Defining the mapper class name
        job.setMapperClass(MaxTemperatureMapper.class);
        // Defining the reducer class name
        job.setReducerClass(MaxTemperatureReducer.class);
        // Defining input Format class which is
        // responsible to parse the dataset
        // into a key value pair
        job.setInputFormatClass(TextInputFormat.class);
        // Defining output Format class which is
        // responsible to parse the dataset
        // into a key value pair
        job.setOutputFormatClass(TextOutputFormat.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
        }
        }







import java.io.IOException;
import java.util.HashMap;

import org.apache.hadoop.conf.*;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.HashMap;

        public class MatrixMultiply {

                public static void main(String[] args) throws Exception {
                if (args.length != 2) {
                    System.err.println("Usage: MatrixMultiply <in_dir> <out_dir>");
                    System.exit(2);
                }
                Configuration conf = new Configuration();
                // M is an m-by-n matrix; N is an n-by-p matrix.
                conf.set("m", "1000");
                conf.set("n", "100");
                conf.set("p", "1000");
                @SuppressWarnings("deprecation")
                        Job job = new Job(conf, "MatrixMultiply");
                job.setJarByClass(MatrixMultiply.class);
                job.setOutputKeyClass(Text.class);
                job.setOutputValueClass(Text.class);

                job.setMapperClass(Map.class);
                job.setReducerClass(Reduce.class);

                job.setInputFormatClass(TextInputFormat.class);
                job.setOutputFormatClass(TextOutputFormat.class);

                FileInputFormat.addInputPath(job, new Path(args[0]));
                FileOutputFormat.setOutputPath(job, new Path(args[1]));

                job.waitForCompletion(true);
                }
                public class Map
                  extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text,
Text, Text> {
                        @Override
                        public void map(LongWritable key, Text value, Context context)
                                        throws IOException, InterruptedException {
                                Configuration conf = context.getConfiguration();
                                int m = Integer.parseInt(conf.get("m"));
                                int p = Integer.parseInt(conf.get("p"));
                                String line = value.toString();
                                // (M, i, j, Mij);
                                String[] indicesAndValue = line.split(",");
                                Text outputKey = new Text();
                                Text outputValue = new Text();
                                if (indicesAndValue[0].equals("M")) {
                                        for (int k = 0; k < p; k++) {
                                                outputKey.set(indicesAndValue[1] + "," + k);
                                                // outputKey.set(i,k);
                                                outputValue.set(indicesAndValue[0] + "," + indicesAndValue[2]
                                                                + "," + indicesAndValue[3]);
                                                // outputValue.set(M,j,Mij);
                                                context.write(outputKey, outputValue);
                                        }
                                } else {
                                        // (N, j, k, Njk);
                                        for (int i = 0; i < m; i++) {
                                                outputKey.set(i + "," + indicesAndValue[2]);
                                                outputValue.set("N," + indicesAndValue[1] + ","
                                                                + indicesAndValue[3]);
                                                context.write(outputKey, outputValue);
                                        }
                                }
                        }
                }
                        public class Reduce
                  extends org.apache.hadoop.mapreduce.Reducer<Text, Text, Text, Text> {
                        @Override
                        public void reduce(Text key, Iterable<Text> values, Context context)
                                        throws IOException, InterruptedException {
                                String[] value;
                                //key=(i,k),
                                //Values = [(M/N,j,V/W),..]
                                HashMap<Integer, Float> hashA = new HashMap<Integer, Float>();
                                HashMap<Integer, Float> hashB = new HashMap<Integer, Float>();
                                for (Text val : values) {
                                        value = val.toString().split(",");
                                        if (value[0].equals("M")) {
                                                hashA.put(Integer.parseInt(value[1]), Float.parseFloat(value[2]));
                                        } else {
                                                hashB.put(Integer.parseInt(value[1]), Float.parseFloat(value[2]));
                                        }
                                }
                                int n = Integer.parseInt(context.getConfiguration().get("n"));
                                float result = 0.0f;
                                float m_ij;
                                float n_jk;
                                for (int j = 0; j < n; j++) {
                                        m_ij = hashA.containsKey(j) ? hashA.get(j) : 0.0f;
                                        n_jk = hashB.containsKey(j) ? hashB.get(j) : 0.0f;
                                        result += m_ij * n_jk;
                                }
                                if (result != 0.0f) {
                                        context.write(null,
                                                        new Text(key.toString() + "," + Float.toString(result)));
                                }
                        }
                }
                } 




create table student (ID int, Name string, Age int)
➢ partitioned by(Course string)
➢ row format delimited
➢ fields terminated by ‘,’;




load data local inpath ‘/home/cloudera/Documents/Student.csv’ into table student
➢ partitioned by(Course string)




select country, sum(salary) from empgroup group by country having
sum(salary)>50000;



Select * from employee sort by salary desc;



insert overwrite table zipcode partition(state) select -------- fromzipcode;


alter table zipcode partition (state ='Al') rename to partition (state='NY');